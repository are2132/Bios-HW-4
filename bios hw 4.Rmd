---
title: "Bios Methods Homework 4"
author: "Alison Elgass"
output: pdf_document
---

# Problems 1 & 2

See hand-written scanned section to follow...
\newpage

```{r include=FALSE}
library(tidyverse)
library(readxl)
library(faraway)
library(broom)
library(dplyr)
```

# Problem 3
```{r}
brain_data = read_excel(path = "./Brain.xlsx") %>% 
  janitor::clean_names()

plot(brain_data$glia_neuron_ratio, brain_data$ln_brain_mass, 
     xlab = "ln brain mass", 
     ylab = "glia-neuron ratio")

nonhuman = brain_data %>% filter(species != "Homo sapiens")
```

### Part A - Regression
```{r}
brain_reg = lm(glia_neuron_ratio ~ ln_brain_mass, data = nonhuman)
summary(brain_reg) 
```
The regression equation is 	$\hat{Y}$ = 0.1637 + 0.18113**X~i~**

### Part B - Prediction
For humans, ln(brain mass) = 7.22, so the regression equation would predict a glia neuron ratio of 0.1637 + 0.18113(7.22) = 1.471  

### Part C - Prediction Interval
The most meaningful estimate would be a prediction interval rather than a confidence interval at the given brain mass.  

### Part D - 95% Interval
95% PI = $\hat{\beta}$~0~ + $\hat{\beta}$~1~X~h~ +- (t~n-2,1-$\alpha$/2)(se($\hat{\beta}$~0~ + $\hat{\beta}$~1~X~h~))  
  
where se = $\sqrt{}$ MSE ((1/n) + ((x~h~ - $\overline{x}$)/
$\sum_{i=1}^{n}$ x~i~ - $\overline{x}$) + 1)  

```{r}
xh = 7.22
predict.lm(brain_reg, data_frame(ln_brain_mass = xh),
        interval = "predict", level = 0.95)
```

We find that our 95% prediction interval for the human glia-neuron ratio is (1.036, 1.907). The actual measured ratio is 1.65, which is within this range, so we conclude that humans are not abnormal compared to other primates.  
  
Since the human data point is somewhat of an outlier we should be cautious since the regression line might not accurately apply to a value of ln(brain mass) that is so high  


# Problem 4
```{r}
heart_data = read_csv(file = "./HeartDisease.csv")

heart_data %>% 
  count(gender)
hist(heart_data$age)
summary(heart_data$ERvisits)
summary(heart_data$totalcost)
```

### Part A - Description
The dataset contains data on 788 patients who made insurance claims for coronary heart disease. The main predictor is the number of ER visits, and the main outcome is total cost. Covariates include age, gender, number of complications, and condition duration.  

### Part B - Total Cost
Altering total cost to ln(totalcost) makes a more normal distribution
```{r}
hist(heart_data$totalcost)
hist(log(heart_data$totalcost))
```

### Part C - Complications Factor
```{r}
new_heart = heart_data %>% 
  filter(totalcost != 0) %>% 
  mutate(  
    #one 3 in row 79
    compbin = factor(complications, 
                     levels = c(0,1,3), labels = c(0,1,1)),
    lncost = log(totalcost)
  )
  
#str(new_heart$compbin)
#new_heart$compbin[79]
```

### Part D - SLR
Note that in creating our `new_heart` dataset above, we also excluded 3 data points where cost was 0, since these will not work with a log transformation. This seems reasonable since these ER costs are likely missing or were not recorded by the insurance company, so they are not helpful for our regression analysis.
```{r}
slr_heart = lm(lncost ~ ERvisits, data = new_heart)
plot(new_heart$ERvisits, new_heart$lncost, 
     xlab = "# ER visits", 
     ylab = "ln total cost")
abline(slr_heart, lwd = 2, col = 2)

summary(slr_heart)
b1_simple = summary(slr_heart)$coefficients[2,1] #0.2267
```

The regression equation is 	ln$\hat{Y}$ = 5.537 + 0.226**X~i~**  
These results are highly significant, as a test of $\hat{\beta}$~1~ = 0 gives the test statistic 9.46 which is > t~785-2,1-0.95/2~ = `r qt(0.975,783)` so we conclude the slope is not 0.  
Our estimated slope is 0.2267, meaning that for every 1 additional ER visit, we would predict that total cost will increase by 100(e^0.2267^ - 1) = 25.4%.   

### Part E - MLR
```{r}
mlr_heart = lm(lncost ~ ERvisits + compbin, data = new_heart)
summary(mlr_heart)
b1_mult = summary(mlr_heart)$coefficients[2,1] #0.2046
```
Using multiple linear regression, our regression equation is 	
ln$\hat{Y}$ = 5.5211 + 0.2046**X~i1~** + 1.686**X~i2~**  
where X~1~ = # ER visits, and X~2~ = whether patient experienced complications (reference category = 0, no complications)


**i. Testing for interaction**
To test for interaction, we run a MLR with an interaction term  
```{r}
mlr_interact = lm(lncost ~ ERvisits*compbin, data = new_heart)
summary(mlr_interact)
-0.1 + c(-1,1)*(0.09483)*qt(0.975,781)
```
This gives the model
ln$\hat{Y}$ = 5.499 + 0.211**X~i1~** + 2.18**X~i2~** - 0.1**X~i1~X~i2~**  
  
The interaction term = -0.1, with a t-value that is insignifcant.  
Also the confidence interval would be -0.1 +- (0.09483)(t~0.975,781~) = 
(-0.286, 0.862) which includes zero, so we conclude there is not a significant interaction between ER visits and complication status.
  
We can also look at the graph of ln(cost) vs. # ER visits, stratified by compbin = 0 or 1, and the respective slopes of the regression lines
``` {r}
#make a plot of slr lines, stratified by compbin
plot(new_heart$ERvisits, new_heart$lncost, 
     xlab = "# ER visits", 
     ylab = "ln total cost")
heart0 = new_heart %>% filter(compbin == 0)
heart1 = new_heart %>% filter(compbin == 1)
lm(lncost ~ ERvisits, data = heart0) %>% abline(lwd = 2, col=2)
lm(lncost ~ ERvisits, data = heart1) %>% abline(lwd = 2, col = 3)
```

For the 742 observations without complications (compbin = 0, in red),  $\hat{\beta}$~1~ = 0.211  
For the remaining 43 observations with complications (compbin = 1),
$\hat{\beta}$~1~ = 0.112  


**ii. Testing for confounding**
To test for confounding, we compare the SLR (ER visits as the only predictor of cost) and MLR (adding in/adjusting for complications)  
```{r}
(b1_mult - b1_simple)/b1_simple #decreases by 9.75%
```
In the simple linear regression, we found $\hat{\beta}$~1~ = 0.2267.  
Adding in the compbin variable and doing a multiple linear regression, now $\hat{\beta}$~1~ = 0.2046, a 9.8% decrease, so I don't believe it is a significant confounder.

**iii. Compbin in the model**
``` {r}
anova(slr_heart, mlr_heart)
partial_r2 = 112.84/2459.8  #= 0.0459
dfS = 785 - 1 - 1
dfL = 785 - 2 - 1
qf(0.95,dfS-dfL,dfL)
```
Testing the effect of the complications variable (X~2~),  
H~0~: $\beta$~2~ = 0   
H~1~: $\beta$~2~ $\neq$ 0  
F = MSR/MSE = 37.598 > F~1,782,0.05~ = 3.85, so we reject the null hypothesis and conclude that $\beta$~2~ is not equal to 0  
  
We also calculate from the previous ANOVA tables the partial R^2^ from the marginal contribution of compbin to be 112.84 / 2459.8 = 0.046.  
In other words, about 4.6% of the variation in cost can be attributed to the complications factorm holding the ER visits variable fixed.  
  
Because of this I would include compbin in our model, so  
ln$\hat{Y}$ = 5.5211 + 0.2046**X~i1~** + 1.686**X~i2~**  
  
### Part F - Final MLR
```{r}
mlr_big = lm(lncost ~ ERvisits + compbin + age + gender + duration,
               data = new_heart)
summary(mlr_big)
```
ln$\hat{Y}$ = 6.05 + 0.176**X~i1~** + 1.492**X~i2~** - 0.022**X~i3~** - 0.118**X~i4~** + 0.0055**X~i5~**  
  
where  
X~i1~ = # ER visits  
X~i2~ = complications (yes compared to no)  
X~i3~ = age (years)  
X~i4~ = gender (male compared to female)  
X~i5~ = duration (days)  
  
The adjusted R^2^ is 0.2633, meaning 26.3% of the variation in cost can be attributed to these covariates as in this model.  
  
### Which model to use?
```{r}
a1 = lm(lncost ~ ERvisits + compbin + age, data = new_heart)
a2 = lm(lncost ~ ERvisits + compbin + gender, data = new_heart)
a3 = lm(lncost ~ ERvisits + compbin + duration, data = new_heart)
tibble("Added Covariate" =
         c("simple","compbin","age","gender","duration"),
       "Adj R2" =
         c(summary(slr_heart)$adj.r.squared,
         summary(mlr_heart)$adj.r.squared,
         summary(a1)$adj.r.squared, #age
         summary(a2)$adj.r.squared, #gender
         summary(a3)$adj.r.squared) #duration
       ) %>% knitr::kable()
```

I would include the complications and duration covariates to adjust the relationship between cost vs. ER visits for these factors. These increase the adjusted R^2^ value. Gender and age do not appear to have a significan affect. It's interesting to note that in the large model, age comes back with a significant p-value (0.0103), however because it does not affect the R^2^ I would not include it.  

```{r}
mlr_final = lm(lncost ~ ERvisits + compbin + duration, data = new_heart)
summary(mlr_final)
```

Our **final model** then is  
ln$\hat{Y}$ = 4.76 + 0.171**X~i1~** + 1.529**X~i2~** + 0.005**X~i3~**  
  
Using the formula % change in Y = 100(e^$\beta$~1~^ - 1),  
we conclude that an additional ER visit increases total cost by about 18%, having complications increases it by 361%, and an additional day of stay increases it by 0.54%.


